---
title: "Introducción a las redes neuronales"
date: Sun Jul 20 2025 19:12:04 GMT+0100 (hora estándar de Europa central)
categories: ["Programación"]
image: "/images/redes-neuronales.webp"
image-mini: "/images/redes-neuronales-mini.webp"
---

En el [artículo anterior](/articles/posts/20250622.html) pudimos revisar en qué consisten las neuronas artificiales, y además vimos cómo poder implementar una neurona en JavaScript. Si aún no lo has leído, te recomiendo que empieces por ahí para entender la base de lo que vamos a ver ahora.

Si aún así quieres empezar a leer este artículo, a modo de resumen, en el anterior hablábamos de neuronas como pequeñas funciones matemáticas que ajustaban pesos y sesgos para intentar dar una salida lo más precisa posible a partir de unas entradas. Hasta ahí todo sencillo.

Pero, si intentamos resolver un problema tan simple como la operación **XOR**, nos encontraremos con una sorpresa. Lo primero de todo, recordemos su tabla de la verdad:

```text
A | B | Salida
0 | 0 | 0
0 | 1 | 1
1 | 0 | 1
1 | 1 | 0
```

El problema de XOR es que **no es linealmente separable**. Una neurona, como la que programamos en el artículo anterior, sólo puede establecer un límite mediante una recta (o un plano si hablamos de más dimensiones) para separar las salidas, o dicho de otra forma, sólo puede crear separaciones lineales entre las entradas y las salidas. En cambio, las capas ocultas permiten que la red combine múltiples límites simples para formar decisiones más complejas y precisas.

## ¿Qué es una red neuronal?

Una **red neuronal artificial** es un conjunto de neuronas conectadas entre sí que trabajan juntas para procesar información. Se inspiran en cómo funciona el cerebro humano, aunque de forma muy simplificada. Ya vimos, que cada neurona recibe entradas, las transforma aplicando una función matemática (lo que llamamos activación) y genera una salida. Pues bien, en una red neuronal, esta salida llega a otras neuronas conectadas. Al unir muchas neuronas, se consigue que la red pueda aprender patrones complejos que una sola neurona no es capaz de distinguir.

Las redes al igual que las cebollas, como diría Shrek, se organizan en **capas**:

- **Capa de entrada**: Recibe los datos iniciales. Por ejemplo, si queremos reconocer imágenes, la capa de entrada recibe los píxeles.
- **Capas ocultas**: Son las encargadas de procesar la información internamente. Cada neurona en estas capas intenta encontrar relaciones, patrones o combinaciones que sean útiles para llegar a una salida correcta. Su nombre viene de que no vemos directamente sus resultados como usuarios.
- **Capa de salida**: Es la que tiene como entradas los resultados de las capas ocultas, y según estados resultados da la respuesta final.

Las redes pueden tener tantas capas ocultas como queramos, pero incluso con una capa podemos resolver problemas como el XOR, como veremos a continuación. Ya que esta aproximación nos permite comenzar a usar la IA como realmente la conocemos, dejando atrás los problemas muy sencillos que podía resolver una única neurona, para pasar a resolver problemas más complejos como:

- Reconocimiento de imágenes
- Traducción automática
- Diagnóstico médico
- Coches autónomos

Y un largo etcétera de posibilidades.

## Construyendo nuestra primera red neuronal

Para resolver la operación XOR, nuestra red neuronal va a ser muy sencilla:

- La **capa oculta** tendrá dos neuronas.
- Y tendremos una **neurona final** de salida.

Esto es suficiente para resolver XOR. Cada neurona de la capa oculta aprenderá a distinguir una de las condiciones necesarias para que XOR funcione. La neurona de salida simplemente combinará esa información.

Para entender cómo "mapeamos" de las neuronas ocultas a la de salida, piensa que cada neurona oculta detecta un patrón concreto (por ejemplo, que A sea 1 y B sea 0). La neurona de salida aprenderá a usar esas "pistas" que proporcionan las neuronas de la capa oculta para decidir su salida.

## Código y explicación paso a paso

### Clase Neuron

En el artículo anterior ya vimos la clase Neuron. Aquí os pongo que métodos públicos tiene a modo de recordatorio:

```javascript
class Neuron {
  /**
   * "Activa" la neurona a partir de las entradas proporcionadas, o
   * dicho de otro modo, hace que la neurona nos dé un resultado.
   */
  activate(inputs) { }

  /**
   * Se encarga del entrenamiento de la neurona dado un conjunto de
   * datos de entrada con sus correspondientes salidas y el número de
   * veces que hay que iterar los datos para "consolidar" esos resultados.
   */
  training(dataset, epochs) { }

  /**
   * Este método es muy importante, ya que ajusta los pesos y el sesgo
   * para que la neurona pueda "predecir" mejor los resultados.
   */
  adjust(inputs, gradient) { }
}
```

### Definiendo la red neuronal

El constructor simplemente inicializa la capa oculta y genera la neurona de salida. Aunque sólo es necesario dos neurona para resolver la operación XOR, ya hemos dejado el código genérico para recibir cualquier número de neuronas.

```javascript
class NeuralNetowrk {
  constructor(inputsNumber, activationFunction, derivateFunction, neuronsNumber, learningRate = 0.1) {
    this.#derivateFunction = derivateFunction;

    this.#hiddenNeurons = [];
    for (let i = 0; i < neuronsNumber; i++) {
      this.#hiddenNeurons.push(
        new Neuron(
          inputsNumber, activationFunction,
          this.#derivateFunction, learningRate
       ));
     }

    // Create output neuron, which takes hidden neuron outputs as inputs
    this.#outputNeuron = new Neuron(
      neuronsNumber, activationFunction, this.#derivateFunction,
      learningRate
    );
  }
}
```

El método que activa la red neuronal, difiere del de una neurona. En este caso activaremos todas las neuronas de la capa oculta, y una vez obtenidos sus resultados, será lo que pasemos a la neurona de salida para que nos proporcione el resultado final:

```javascript
activate(inputs) {
  const hiddenOutputs = this.#hiddenNeurons.map(
    neuron => neuron.activate(inputs)
  );
  return this.#outputNeuron.activate(hiddenOutputs);
}
```

El método de entrenamiento público, será exactamente el mismo que el de una neurona ya que simplemente iterará el conjunto de datos el número de pasos (épocas) necesarias.

En cada época, comenzaremos con el entrenamiento, algo que a nivel de código tampoco es demasiado complejo en su método principal:

```javascript
#train(inputs, expectedOutput) {
  // Esta parte es muy similar a lo que hace una neurona, que es obtener
  // una predicción tanto para las capas ocultas como para la neurona de
  // salida.
  const hiddenOutputs = this.#hiddenNeurons.map(n => n.activate(inputs));
  const computedOutput = this.#outputNeuron.activate(hiddenOutputs);

  // Igualmente, calculamos la desviación del error, basándonos en la
  // predicción generada por la neurona de salida.
  const adjustValue = calculateAdjust(
    expectedOutput, computedOutput, this.#derivateFunction
  );

  // Ahora debemos propagar la desviación del error para hacer ajustes y
  // corregir tanto la neurona de salida como las neuronas de la capa
  // oculta
  this.#adjustWeights(hiddenOutputs, inputs, adjustValue);
}
```

Y aquí comienza lo complicado, ya que como habréis visto en los comentarios del código anterior, es necesario propagar la desviación de error, no sólo de una neurona, sino a todas las de la red:

```javascript
#adjustWeights(hiddenOutputs, inputs, adjust) {
  // Mandamos el ajuste a la neurona de salida
  this.#outputNeuron.adjust(hiddenOutputs, adjust);

  // Aquí mandamos el ajuste a las neuronas de la capa oculta. No podemos 
  // usar el ajuste de la neurona de salida directamente, sino que tenemos
  // que realizar dicho ajuste en base al peso de cada neurona de la capa
  // oculta, para que la desviación sea acorde con dicha neurona. A este
  // valor derivado del ajuste de la neurona de salida le llamamos
  // gradiente.
  for (let index = 0; index < this.#hiddenNeurons.length; ++index) {
    const hiddenNeuron = this.#hiddenNeurons[index];

    const gradient = this.#computeGradient(
      hiddenNeuron.Output, this.#outputNeuron.Weights[index], adjust
    );

    hiddenNeuron.adjust(inputs, gradient);
  }
}

#computeGradient(output, weight, adjust) {
  return this.#derivateFunction(output) * weight * adjust;
}
```

### Entrenando la red

El entrenamiento es muy parecido al que realizamos con una sóla neurona, así que hemos aprovechado para hacer que la API pública de la red neuronal sea similar a la de una neurona.

```javascript
const xorData = [
  { inputs: [0, 0], output: 0 },
  { inputs: [0, 1], output: 1 },
  { inputs: [1, 0], output: 1 },
  { inputs: [1, 1], output: 0 }
];

const neuralNetwork = new NeuralNetwork(
  2, activationSigmoid, derivateSigmoid, 2, 0.1
);
neuralNetwork.training(xorData, 50000);

// Si probamos ahora a imprimir los resultados (recordad que no nos
// devuelve un valor exacto sino una aproximación, de ahí que usemos la
// función Math.round().
console.log(Math.round(neuralNetwork.activate([0, 0]))); // Imprime 0
console.log(Math.round(neuralNetwork.activate([0, 1]))); // Imprime 1
console.log(Math.round(neuralNetwork.activate([1, 0]))); // Imprime 1
console.log(Math.round(neuralNetwork.activate([1, 1]))); // Imprime 0
```

Bien, el código que hemos visto nos permite generar una red neuronal de forma muy sencilla. Hay formas de desarrollarla mucho más avanzadas y correctas como sería usar un **gradiente descendente**, pero como siempre digo, eso será tema para otros artículos.

## Resumiendo...

- Una **neurona aislada** es muy limitada, por lo que no sirve para operaciones complejas.
- Cuando unimos varias neuronas y les permitimos aprender juntas generamos una **red neuronal**.
- Incluso sin grandes frameworks, podemos construir pequeños ejemplos en JavaScript (o cualquier otro lenguage que se te ocurra) para entender las bases.
- Hemos pasado de una neurona que "resolvía" operaciones muy sencillas, a una pequeña red que es capaz de resolver problemas imposibles para una sola.

Espero que os haya servido para entender un poco más las bases de este apasionante mundillo.

El código del proyecto ya está actualizado en [GitHub](https://github.com/jafs/mini-ai), con la red neuronal. Podéis descargarlo, mejorarlo, experimentar y todo lo que se os ocurra.

Muchas gracias si habéis llegado hasta aquí. Nos vemos en el próximo artículo.
